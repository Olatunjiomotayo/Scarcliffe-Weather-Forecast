---
title: "Data Science 7006"
author: "Ismail Olatunji - 2315053"
date: "2024-05-14"
output:
  html_document: default
  pdf_document: default
  word_document: default
Student ID: 2315053
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(naniar)
library(zoo)
library(DescTools)
library(caTools)
library(lmtest)
library(car)
library(olsrr)
library(e1071)
library(randomForest)
library(tsoutliers)
library(tseries)
library(forecast)
```


## Data Science Assessment 2

### Restructuring my Dataset

Let's start by importing the Weather data

We will restructure the data here
```{r cars}
getwd()
setwd("C:/Users/user/OneDrive/Documents/DAT7006/ASS 2")

#Read the CSV file and skipp the first row
Weather_data <- read.csv("Weather Research & Forecasting Dataset 2018.csv", header = FALSE, skip = 1)

View(Weather_data)


#we set the column names as character and skipped first rol of the Weather_data
colnames(Weather_data) <- as.character(unlist(Weather_data[1, ]))
Weather_data <- Weather_data[-1, ]


View(Weather_data)

#made a subset where XLAT == 53.227 & XLONG == -1.224
Scarcliffe_data <- subset(Weather_data, XLAT == 53.227 & XLONG == -1.224)
View(Scarcliffe_data)

#I removed the first and second columns (XLAT & XLONG) in order to be able to split 
#the columns to groups of 10 each as the first 10 columns are iterated all over for 
#the remaining columns.

ScarcliffeXX <- Scarcliffe_data[, -(1:2)]

View(ScarcliffeXX)

#Splitted Mansfield_columns dataframe into groups with 10 columns each
Scarcliffe_columns <- split.default(ScarcliffeXX, rep(1:248, each = 10))


View(Scarcliffe_columns)

#used lapply function combine the XLAT & XLONG columns back and naming each columns
Scarcliffe_Join <- lapply(Scarcliffe_columns, function(x) {
  Scarcliffe_bind <- cbind(Scarcliffe_data[, 1:2], x)
  colnames(Scarcliffe_bind) <- c("XLAT", "XLONG", "TSK", "PSFC", "u10", "v10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS")
  
  return(Scarcliffe_bind)
})

View(Scarcliffe_Join)

#combine the lists of data frames into one single data frame
Scarcliffemain_df <- do.call(rbind, Scarcliffe_Join)
View(Scarcliffemain_df)
```

After restructing the dataset, I will be creating new variable "Datetime" using the time as.POSIXlt

```{r}
#Let's generate the sequence of time stamps with 3-hour intervals
# Define the start date and time using as.POSIXlt
start_date <- as.POSIXlt("01-05-2018 00:00:00", format = "%d-%m-%Y %H:%M:%S")

nrows <- nrow(Scarcliffemain_df)
View(nrows)

# Create a sequence of timestamps with a 3-hour interval for additional rows
timestamp_sequence <- seq(start_date, by = "3 hours", length.out = nrows)

# extract the format, how we want it to be arranged
timestamp <- format(timestamp_sequence, "%d-%m-%Y %H:%M:%S")

Scarcliffemain_df$Datetime <- timestamp


# View the resulting data frame
View(Scarcliffemain_df)
head(Scarcliffemain_df)
tail(Scarcliffemain_df)

str(Scarcliffemain_df)
```

The data types are all in character form, I need to change all back to there proper data type


```{r}

#data types - change each column to the right data type

Scarcliffemain_df <- Scarcliffemain_df %>%
  mutate_at(1:12, as.numeric)
View(Scarcliffemain_df)

#convert the Datetime column to date format
Scarcliffemain_df$Datetime <- as.POSIXlt(Scarcliffemain_df$Datetime, format = "%d-%m-%Y %H:%M:%S")

# Now, recheck to be sure they are in right format now
str(Scarcliffemain_df)

# then view
View(Scarcliffemain_df)
```

### Checking for missing values and handling them

```{r}

#check the number of NA we have in the whole dataset
sum(is.na(Scarcliffemain_df))

#gives us breakdown of NA in each columns
colSums(is.na(Scarcliffemain_df))


#I will create a dataframe of the na_values
missing_values <- data.frame(
  Columns = c("XLAT", "XLONG", "TSK", "PSFC", "u10", "v10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS", "Datetime"),
  na_count = c(0, 0, 6, 6, 7, 5, 8, 6, 8, 6, 6, 9, 0)
)
View(missing_values)

# I filter out rows with 0 NA_count as they won't be needed
missing_values1 <- missing_values[missing_values$na_count > 0, ]
missing_values1

#view filtered dataframe after removing rows/values with 0
View(missing_values1)

```

### Visualisation of the missing values

The bar chart displays the missing values in each columns

The second plot shows the percent of missing values in each column
```{r}
#Below is the plot for our missing values on each column

ggplot(data = missing_values1, aes(x = na_count, y = Columns)) + 
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Chart of Missing Values per Column", 
       x = "Missing Values", 
       y = "Columns")

#this will show us the percentage of missing values in each column
vis_miss(Scarcliffemain_df)

View(Scarcliffemain_df)  # this gives us overview of all our missing values
```

### Handling of missing values

na.approx function was called to action here to handle the NA values

```{r}
# Let's handle all missing values column by column
colSums(is.na(Scarcliffemain_df))


# na.approx function will be used to interpolate NA values in each column
Scarcliffemain_df1 <- Scarcliffemain_df %>%
  mutate(across(c(TSK, PSFC, u10, v10, Q2, RAINC, RAINNC, SNOW, TSLB, SMOIS), na.approx))

View(Scarcliffemain_df1)

# Let's recheck to be sure we don't have missing values anymore
# All missing values are now handled, which means we have no missing value anymore
colSums(is.na(Scarcliffemain_df1))

#let's recheck our missing values percentage again
vis_miss(Scarcliffemain_df1)
#the visualisation above shows that we don't have any missing value anymore
```


### Calculate Windspeed Using X and Y Components of Wind (u10, v10)

The Windspeed variable will be added to the dataset

```{r}
# Calculate wind speed using the Pythagorean theorem, I will use mutate function to create a new
# column "Windspeed"

Scarcliffemain_df1 <- Scarcliffemain_df1 %>%
  mutate(Windspeed = sqrt(u10^2 + v10^2))
head(Scarcliffemain_df1)
```


### Dropping of Columns

I will be dropping u10, v10, RAINNC, SNOW columns, they are not needed for my analysis

```{r}
Scarcliffenew_df <- Scarcliffemain_df1 %>%
  select(-u10, -v10, -RAINNC, -SNOW)


# We are now left with 9 columns, XLAT, XLONG, TSK, PSFC, Q2, RAINC, TSLB, SMOIS Datetime, Windspeed

```


### Conversion of Units

I will be doing conversion of some parametres to common UK standard

```{r}
Scarcliffenew_df <- Scarcliffenew_df %>%
  mutate(TSK = TSK - 273.15,         # conversion of TSK from Kelvin to Celsius
         TSLB = TSLB - 273.15,       # conversion of TSLB from Kelvin to Celsius
         PSFC = PSFC / 100,          # conversion of PSFC from Pascal to hPa
         Q2 = Q2 * 1000)             # conversion of Q2 from kg/kg to g/kg

head(Scarcliffenew_df)
tail(Scarcliffenew_df)
```


### Outliers

#### 1. Detecting outliers in TSK column using IQR

What we need to do next is to detect outliers and handle them appropriately

```{r}

quantile(Scarcliffenew_df$TSK)
summary(Scarcliffenew_df$TSK)
TSK_IQR <- IQR(Scarcliffenew_df$TSK) #this is simply Q3 - Q1, Q3 = 19.175 and Q1 = 8.725
TSK_IQR

#Q3 <- quantile(Scarcliffenew_df$TSK, 0.75)
#Q1 <- quantile(Scarcliffenew_df$TSK, 0.25)

#let's get our upper and lower bounds in TSK column
upper_bound <- quantile(Scarcliffenew_df$TSK, 0.75) + 1.5 * TSK_IQR
upper_bound # upper bound is 34.85 which means, any figure above this is an outlier

lower_bound <- quantile(Scarcliffenew_df$TSK, 0.25) - 1.5 * TSK_IQR
lower_bound 

#lower bound is -6.95, any value below the lower bound is an outlier

#Remove outliers

TSK_outliers <- Scarcliffenew_df[Scarcliffenew_df$TSK < lower_bound | Scarcliffenew_df$TSK > upper_bound]
print(TSK_outliers)

boxplot(Scarcliffenew_df$TSK, main = "TSK Boxplot with Outliers")

#From the IQR detection method and the boxplot visualisation, it is clear that TSK column has no outlier
```


### 2. Detecting and handling of outliers in PSFC column using Tukey's Fences method

```{r}
PSFC_outliers <- boxplot.stats(Scarcliffenew_df$PSFC)$out
print(PSFC_outliers)

boxplot(Scarcliffenew_df$PSFC, main = "PSFC Boxplot with Outliers")
#### from the visualisation, it is clear we have 5 outliers in PSFC Column, which 989.54 987.48 985.84 986.93 989.90
```

### 3. Detecting and handling outliers in Q2 column

```{r}
Q2_outliers <- boxplot.stats(Scarcliffenew_df$Q2)$out
print(Q2_outliers)

boxplot(Scarcliffenew_df$Q2, main = "Boxplot Visualisation of Q2 Column")
# No outliers detected in Q2 Column
```

###  4. Detecting and handling outliers in RAINC column

```{r}
RAINC_outliers <- boxplot.stats(Scarcliffenew_df$RAINC)$out
print(RAINC_outliers)

# Histogram of RAINC values
hist(Scarcliffenew_df$RAINC, main = "Histogram of Convective Rain (Accumulated)", xlab = "Rainfall in mm", col = "blue")

# Leaving the outliers in order to ensure that the analysis reflects the true 
# variability in rainfall patterns.
```


### 5. Detecting and handling outliers in TSLB column

```{r}
TSLB_outliers <- boxplot.stats(Scarcliffenew_df$TSLB)$out
print(TSLB_outliers)

boxplot(Scarcliffenew_df$TSLB, main = "Boxplot of Soil Temperature TSLB")
```


### 6. Detecting and handling outliers in Windspeed column

```{r}
Windspeed_outliers <- boxplot.stats(Scarcliffenew_df$Windspeed)$out
print(Windspeed_outliers)   

boxplot(Scarcliffenew_df$Windspeed, main = "Boxplot of Windspeed in m/s")

# my outliers here are 8.570298 8.174350 9.213577 9.374433, but I will not be dropping any of them
```

After getting outliers in some columns, I decided not to remove them because they are true outliers; natural variations.They are genuine observations and very vital. 



# STATISTICAL ANALYSIS

### Univariate Analysis

I will be doing Univariate analysis of Convective rain (Accumulated precipitation) - RAINC

To determine the RAINC central tendency and distribution, Summary() function in R will give us the basic descriptive statistical analysis such as the minimum value, 1st quartile, Mean, Median, 3rd quartile and the Maximum number. 

Histogram will also be used to visualise RAINC frequency distribution to understand its distribution shape

Additionally, I will use Shapiro-Wilk to test the normality of RAINC to determine if it follow a normal distribution.

```{r}
summary(Scarcliffenew_df$RAINC)

hist(Scarcliffemain_df1$RAINC, 
     main = "Distribution of Rainfall in Scarcliffe", 
     xlab = "Accumulated Precipitation (mm)", col = "blue")

# Shapiro-Wilk normality test

shapiro.test(Scarcliffenew_df$RAINC)
```

a. The histogram shows a highly skewed distribution with a significant peak at 0.0 mm, indicating that the most frequent observation is no precipitation. There are a few smaller bars to the right, showing that there are fewer occurrences of higher precipitation values. This distribution suggests that over the period or area studied, dry conditions are much more common than wet conditions. The shape of the distribution, heavily skewed with a long tail to the right, is typical for precipitation data, where many dry days and a few very wet days are common.

b. The Shapiro-Wilk test results show that RAINC deviates from a normal distribution. This indicates that the test and model hypotheses based on normally distributed data are not met. It is very obvious to arrive at this conclusion mere looking at the histogram

We reject the null hypothesis

# Bivariate Analysis

Let's explore the relationship between temperature and Soil Temperature in Scarcliffe by plotting a scatter plot.

```{r}
ggplot(data = Scarcliffenew_df, aes(x = TSK, y = TSLB)) +
  geom_point(size = 2) +
  labs(x = "Surface Temperature (C)", y = "Soil Temperature (C)", title = "Scatter Plot of Surface Temperature vs. Soil Temperature")

# Check for Pearson's correlation coefficient
cor.test(Scarcliffenew_df$TSK, Scarcliffenew_df$TSLB, method = "pearson")
```

The plot above shows a scatter plot of surface temperature on the x-axis plotted against the soil temperature on the y-axis. The scattered data points on the graph clearly demonstrate a pattern that shows a rise in soil temperature in relation to surface temperature.

Pearson correlation test shows the direction and strength of linear connection between surface temperature and soil temperature. From the result, it means surface temperature and soil temperature have strong correlation in the positive direction.


# Multivariate Analysis

To investigate the correlations between a dependent variable, surface temperature (TSK) and several independent variables such as, surface pressure PSFC, humidity Q2, precipitation RAINC, soil temperature TSLB, and wind speed Windspeed), we can design a multiple linear regression model.

```{r}
# Fit a multiple linear regression model

mlr_model <- lm(TSK ~ PSFC + Q2 + RAINC + TSLB + Windspeed, data = Scarcliffenew_df)

summary(mlr_model)
```

This study provides insight into the individual and combined impacts of the independent variables on the dependent variable, as well as the overall model fit and significance.

Basically, what this regression summary means is that both TSLB and Q2 are strong predictor to our dependant variable, TSK, also windspeed is a strong predictor too, but not as strong as TSLB and Q2, other variables, PSFC and RAINC have no significance at all.

The multiple R-squared value is 0.4363 which means the variables are moderately correlated



# MACHINE LEARNING

### 1. Multiple Linear Regression

We will start by spliting the dataset into separate training and testing sets. This allows training the model on one portion of the data and evaluate its performance on the unseen testing data.

```{r}
# Set seed for reproducibility
set.seed(123)

# Split the data into training and testing sets

split <- sample.split(Scarcliffenew_df, SplitRatio = 0.8)
train_set <- subset(Scarcliffenew_df, split == TRUE)
test_set <- subset(Scarcliffenew_df, split == FALSE)


# Select dependent and independent variable for training data
X_train <- train_set[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_train <- train_set$TSK


# Fit the multiple linear regression model
mlr_model <- lm(TSK ~ ., data = train_set[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed", "TSK")])

# Select dependent and independent variables for testing data
X_test <- test_set[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_test <- test_set$TSK

# Make predictions on the test data
mlr_pred <- predict(mlr_model, newdata = X_test)
mlr_pred

# Calculate RMSE
rmse <- sqrt(mean((y_test - mlr_pred)^2))
rmse

# Calculate MAE
mae <- mean(abs(y_test - mlr_pred))
print(paste("MAE:", mae))
```


## Let's do series of testing on the model

Let's start by analysing the residuals (the discrepancies between actual and expected values) to look for trends or violations of assumptions. This plot is a set of diagnostic charts typically used to analyse the assumptions and performance of a linear regression model


```{r}
# Let's do some testing on the model
par(mfrow = c(2,2)) 
plot(mlr_model)
```


### Check for Linearity

In this situation, the p-value of 0.916 is bigger than the generally accepted significance level of 0.05. This means that we fail to reject the null hypothesis of linearity. In other words, based on the Rainbow Test, there is no evidence to suggest that the linearity assumption is violated in the multiple linear regression model

```{r}
install.packages("lmtest") # install and load lmtest package
library(lmtest)

raintest(mlr_model)

# p-value is 0.916 which interprets that my model is fit and has a linear relationship
```

### Check for Heteroscedasticity

The p-value = 0.76837, which is higher than the typical significance level of 0.05. This indicates that the constant variance (homoscedasticity) null hypothesis cannot be rejected.

```{r}
install.packages("car")  # install and load the "car" package
library(car)

ncvTest(mlr_model)

# p-value is 0.76837, it means no heteroscedasticity
```

### Normality of Errors/Residuals

p-value is 4.995e-07, which means the normality of errors is not normally distributed. The p-value is exceedingly small, significantly below the standard significance value of 0.05. This means that there is strong evidence to reject the null hypothesis of normality.

```{r}
install.packages('olsrr')
library(olsrr)

ols_plot_resid_hist(mlr_model)
ols_plot_resid_qq(mlr_model)
shapiro.test(mlr_model$residuals) # p>0.05 so distribution is normal
```


The Shapiro-Wilk test indicates that the residuals of the linear regression model lack characteristics of a normal distribution as shown above.
The assumption of normality of residuals is one of the key assumptions of linear regression models. Violation of this assumption can have implications for the validity of statistical inferences and the reliability of the model's predictions.

When the residuals are not normally distributed, it may be necessary to consider alternative modeling approaches, such as transforming the response variable, using robust regression techniques, or exploring non-parametric methods that do not rely on the assumption of normality.


### No Multicollinearity

```{r}
# two or more predictor (IV) variables are highly correlated

vif(mlr_model)  #vif>10 strong multicollinearity

#identify outliers that have too much influence on the model
#influential data points - values above/below 3 are problematic

ols_plot_resid_stud(mlr_model)
```

The VIF values for all of the indipendent variables (PSFC, Q2, RAINC, TSLB, and Windspeed) are less than 10, indicating that the model lacks substantial multicollinearity. Multicollinearity is the condition in which two or more predictor variables are highly linked, resulting in unstable and incorrect coefficient estimations.


### 2. SUPPORT VECTOR REGRESSION

```{r}
# 2. SUPPORT VECTOR REGRESSION

# Install and load the required package

# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Scarcliffenew_df), 0.8 * nrow(Scarcliffenew_df))
train_data <- Scarcliffenew_df[train_indices, ]
test_data <- Scarcliffenew_df[-train_indices, ]

# Separate features and target variable
X_train <- train_data[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_train <- train_data$TSK
X_test <- test_data[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_test <- test_data$TSK

# Train the SVR model
svr_model <- svm(y_train ~ ., data = cbind(X_train, y_train), kernel = "radial", type = "eps-regression")

# Make predictions on the test data
svr_pred <- predict(svr_model, newdata = X_test)

# Calculate RMSE
svr_rmse <- sqrt(mean((y_test - svr_pred)^2))
print(svr_rmse)


# Calculate MAE
svr_mae <- mean(abs(y_test - svr_pred))
print(svr_mae)


# Residual Plots
ggplot(data.frame(y_test, svr_pred), aes(x = svr_pred, y = y_test - svr_pred)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Temperature (SVR)", y = "Residuals")



# Predicted vs. Actual Plots
ggplot(data.frame(y_test, svr_pred), aes(x = y_test, y = svr_pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Actual Temperature", y = "Predicted Temperature (SVR)")
```

### 3. RANDOM FOREST REGRESSION

```{r}
# Install and load the required package

# Split the data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(Scarcliffenew_df), 0.8 * nrow(Scarcliffenew_df))
train_data <- Scarcliffenew_df[train_indices, ]
test_data <- Scarcliffenew_df[-train_indices, ]

# Separate features and target variable
X_train <- train_data[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_train <- train_data$TSK
X_test <- test_data[, c("PSFC", "Q2", "RAINC", "TSLB", "Windspeed")]
y_test <- test_data$TSK

# Train the Random Forest Regression model
rf_model <- randomForest(y_train ~ ., data = cbind(X_train, y_train), ntree = 500)

# Make predictions on the test data
rf_pred <- predict(rf_model, newdata = X_test)

# Calculate RMSE
rf_rmse <- sqrt(mean((y_test - rf_pred)^2))
print(rf_rmse)

# Calculate MSE
rf_mae <- mean(abs(y_test - rf_pred))
print(rf_mae)



ggplot(data.frame(y_test, rf_pred), aes(x = rf_pred, y = y_test - rf_pred)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Predicted Temperature (Random Forest)", y = "Residuals")




ggplot(data.frame(y_test, rf_pred), aes(x = y_test, y = rf_pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Actual Temperature", y = "Predicted Temperature (Random Forest)")
```

## TIME SERIES FORECASTING

```{r}

df_ts <- Scarcliffenew_df[, c("TSK", "Datetime")]
myts_data <- ts(data = df_ts$TSK, start = 1, frequency = 8)

plot(myts_data)

#seasonal decomposition
plot(decompose(myts_data))


timeseries_outliers <- tso(myts_data)
timeseries_outliers

acf(myts_data, xlab = "Lag", main = "ACF Plot for TSK")

pacf(myts_data, xlab = "Lag", main = "PACF Plot for TSK")


adf.test(myts_data)

# we have do do differencing to make sure the data is stational
diff_ts_data <- diff(myts_data)

# data is now stational
adf.test(diff_ts_data)

ts_arima <- auto.arima(myts_data)
ts_arima

summary(ts_arima)

# Fit the ARIMA models

arima1 <- arima(diff_ts_data, order = c(1,0,0), seasonal = list(order = c(1,1,0), period = 8))
arima2 <- arima(diff_ts_data, order = c(1,0,2), seasonal = list(order = c(1,1,0), period = 8))
arima3 <- arima(diff_ts_data, order = c(2,0,3), seasonal = list(order = c(2,1,2), period = 8))
arima4 <- arima(diff_ts_data, order = c(1,1,0), seasonal = list(order = c(2,1,0), period = 8))

cat("Arima1AIC: ", arima1$aic, "Arima2AIC: ", arima2$aic, "Arima3AIC: ", arima3$aic, " Arima4AIC: ", arima4$aic)

checkresiduals(arima3)

my_forecast <- forecast(arima3, h=240)
plot(my_forecast)

```

